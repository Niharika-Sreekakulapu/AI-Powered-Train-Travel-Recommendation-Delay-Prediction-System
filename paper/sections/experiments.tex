\section{Experiments and Results}
Because no experiment re‑runs were requested, we report the results present in the repository and documentation. The demo model reports the following metrics on the held‑out test split used in \texttt{train_v2_minimal.py} and \texttt{simple_train.py}:
\begin{table}[h]
\centering
\caption{Summary of reported model performance (demo dataset)}
\begin{tabular}{@{}lcc@{}}
\toprule
Metric & Value & Notes \\
\midrule
Mean Absolute Error (MAE) & 3.47 min & reported in README \\
R\textsuperscript{2} & 0.9826 & reported in README \\
Training time (demo) & $<$ 30 s & single-run, CPU reported \\
Prediction latency & $<$ 100 ms & backend measurement \\
\bottomrule
\end{tabular}
\label{tab:performance}
\end{table}

Feature importance (reported by the saved RandomForest) ranks distance as the most important feature ($\approx$52.2\%), followed by season and month. Figure~\ref{fig:featimp} shows the top features as extracted from the saved \texttt{model_v2.pkl} artifact.

\FloatBarrier
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/feature_importance.png}
  \caption{Feature importance for the RandomForest model (demo).}
  \label{fig:featimp}
\end{figure}
\FloatBarrier

Figure~\ref{fig:scatter} shows predicted vs ground truth delays for the sample dataset and Figure~\ref{fig:coverage} summarizes error coverage vs absolute error thresholds (a calibration‑style diagnostic).

\FloatBarrier
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/pred_vs_true_scatter.png}
  \caption{Predicted vs ground truth delay (scatter).}
  \label{fig:scatter}
\end{figure}
\FloatBarrier
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/error_coverage.png}
  \caption{Error coverage vs absolute error threshold (fraction of examples within threshold).}
  \label{fig:coverage}
\end{figure}
\FloatBarrier

\FloatBarrier
\subsection{Additional diagnostics}
To better understand model behavior and error modes, we include additional diagnostic plots: residual histograms, calibration curves, residual vs.
prediction scatter, and a canonical distance distribution for the demo snapshot. These diagnostics help identify skewed error distributions and miscalibration that may affect recommendations.
\FloatBarrier

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/residual_hist.png}
  \caption{Residual histogram (predicted minus true delay).}
  \label{fig:residhist}
\end{figure}
\FloatBarrier
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/calibration_curve.png}
  \caption{Calibration curve for conformal intervals (nominal vs empirical coverage).}
  \label{fig:calib}
\end{figure}
\FloatBarrier
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/residual_vs_pred.png}
  \caption{Residual vs predicted delays; useful to detect heteroscedastic errors.}
  \label{fig:residpred}
\end{figure}
\FloatBarrier
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/distance_dist.png}
  \caption{Distribution of route distances in the demo snapshot.}
  \label{fig:dist}
\end{figure}
\FloatBarrier

\subsection{Limitations}
The demo dataset is relatively small (few hundred to a few thousand route records depending on the master snapshot) and may not reflect national or cross‑seasonal variation. We recommend larger temporally separated validation sets, external benchmarks, and operational A/B tests before deploying the model broadly.

\subsection{Practical case study}
In the Appendix we include a short case study where the system flags routes with high predicted variability due to imputed RailRadar values; we show how adjusting risk thresholds avoids recommending routes with high uncertainty for risk-averse users.
