\section{Experiments and Results}
Because no experiment re‑runs were requested, we report the results present in the repository and documentation. The demo model reports the following metrics on the held‑out test split used in \texttt{train_v2_minimal.py} and \texttt{simple_train.py}:
\begin{table}[h]
\centering
\caption{Summary of reported model performance (demo dataset)}
\begin{tabular}{@{}lcc@{}}
\toprule
Metric & Value & Notes \\
\midrule
Mean Absolute Error (MAE) & 3.47 min & reported in README \\
R\textsuperscript{2} & 0.9826 & reported in README \\
Training time (demo) & $<$ 30 s & single-run, CPU reported \\
Prediction latency & $<$ 100 ms & backend measurement \\
\bottomrule
\end{tabular}
\label{tab:performance}
\end{table}

Feature importance (reported by the saved RandomForest) ranks distance as the most important feature ($\approx$52.2\%), followed by season and month. Figure~\ref{fig:featimp} shows the top features as extracted from the saved \texttt{model_v2.pkl} artifact.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.48\textwidth]{figs/feature_importance.png}
  \caption{Feature importance for the RandomForest model (demo).}
  \label{fig:featimp}
\end{figure}

Figure~\ref{fig:scatter} shows predicted vs ground truth delays for the sample dataset and Figure~\ref{fig:coverage} summarizes error coverage vs absolute error thresholds (a calibration‑style diagnostic).

\begin{figure}[h]
  \centering
  \includegraphics[width=0.48\textwidth]{figs/pred_vs_true_scatter.png}
  \caption{Predicted vs ground truth delay (scatter).}
  \label{fig:scatter}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.48\textwidth]{figs/error_coverage.png}
  \caption{Error coverage vs absolute error threshold (fraction of examples within threshold).}
  \label{fig:coverage}
\end{figure}

\subsection{Limitations}
The demo dataset is relatively small (few hundred to a few thousand route records depending on the master snapshot) and may not reflect national or cross‑seasonal variation. We recommend larger temporally separated validation sets, external benchmarks, and operational A/B tests before deploying the model broadly.
